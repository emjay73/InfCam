<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <!-- <title>PAPER_TITLE - AUTHOR_NAMES | Academic Research</title> -->
  <title>InfCam</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/camicon.png">
  <link rel="apple-touch-icon" href="static/images/camicon.jpg">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/camicon.jpg",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <!-- <h1 class="title is-2 publication-title">Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</h1> -->
            <h1 class="title is-2 publication-title">
              <span style="color: #2563eb; font-weight: bold;">InfCam</span>: Infinite-Homography as Robust Conditioning for Camera-Controlled </br>Video Generation
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://emjay73.github.io/" target="_blank">Min-Jung Kim</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.co.kr/citations?user=4SCCBFwAAAAJ&hl=ko" target="_blank">Jeongho Kim</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.co.kr/citations?hl=ko&user=Jp-zhtUAAAAJ" target="_blank">Hoiyeong Jin</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://junhahyung.github.io/" target="_blank">Junha Hyung</a>,</span>
                  </span>
              
                  <span class="author-block">
                    <a href="https://sites.google.com/site/jaegulchoo" target="_blank">Jaegul Choo</a>
                  </span>
                  <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <!-- <span class="author-block">KAIST<br>Arxiv</span> -->
                    <!-- <span class="author-block">KAIST AI</span> -->
                    <!-- TODO: Remove this line if no equal contribution -->                    
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    <div style="display: flex; justify-content: center; align-items: center; margin-bottom: 1.5rem;">
                      <img src="static/images/GSAI_preview_image.png" alt="GSAI preview" style="max-width: 30%; height: auto;">                      
                    </div>
                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (Coming Soon)</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/emjay73/InfCam" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop mt-6 mb-6">
    <div id="ours-video-carousel" class="carousel-container" style="position: relative; max-width: 800px; margin: auto;">

      <div class="carousel-videos" style="overflow: hidden; position: relative;">
        <!-- Videos: update the src list below as needed -->
        <video src="static/videos/ours_grid_in-the-wild_1.mp4" class="carousel-video" style="width:100%; display:block;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_in-the-wild_2.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_in-the-wild_3.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_1.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_2.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_3.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_4.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_5_wideFoV.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_6_wideFoV.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_7_narrowFoV.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <video src="static/videos/ours_grid_webvid_8_narrowFoV.mp4" class="carousel-video" style="width:100%; display:none;" autoplay loop muted playsinline></video>
        <!-- Add more <video> elements if needed, the JS will pick them up -->

        <!-- Move the arrows below the video, centered horizontally -->
        <div class="carousel-controls" style="display: flex; justify-content: center; align-items: center; gap: 1.5rem; width: 100%; margin-top: 1rem;">
          <button class="carousel-btn left"
            style="background:rgba(255,255,255,0.85); border:none; border-radius:50%; width:44px; height:44px; display:flex; align-items:center; justify-content:center; box-shadow:0 2px 8px rgba(0,0,0,0.09); transition: background 0.2s;">
            <svg height="26" width="26" viewBox="0 0 32 32" fill="none" stroke="#2563eb" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" style="display:block;">
              <circle cx="16" cy="16" r="15" fill="#fff" stroke="#e6eaf3" stroke-width="2"/>
              <polyline points="19,9 12,16 19,23" fill="none"/>
            </svg>
          </button>
          <button class="carousel-btn right"
            style="background:rgba(255,255,255,0.85); border:none; border-radius:50%; width:44px; height:44px; display:flex; align-items:center; justify-content:center; box-shadow:0 2px 8px rgba(0,0,0,0.09); transition: background 0.2s;">
            <svg height="26" width="26" viewBox="0 0 32 32" fill="none" stroke="#2563eb" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" style="display:block;">
              <circle cx="16" cy="16" r="15" fill="#fff" stroke="#e6eaf3" stroke-width="2"/>
              <polyline points="13,9 20,16 13,23" fill="none"/>
            </svg>
          </button>
        </div>
      </div>

    </div>
    <!-- <h2 class="subtitle has-text-left">
      </br>
      <strong>InfCam Results.</strong> Given a video and a target camera trajectory, InfCam generates a video that faithfully follows the specified camera path. 
      The world coordinate origin is defined by the first frame’s camera pose (highlighted in red). 
      The leftmost column visualizes the backward, arc, and rotational camera trajectories, and the right side shows input–generated video pairs corresponding to each trajectory.
      The rotational trajectory is generated with a shorter focal length to illustrate wide field-of-view generation. The black dashed box in the last row indicates the original field-of-view of the input video.
    </h2> -->
  </div>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      const carousel = document.getElementById('ours-video-carousel');
      if (!carousel) return;
      const videos = carousel.querySelectorAll('.carousel-video');
      const btnLeft = carousel.querySelector('.carousel-btn.left');
      const btnRight = carousel.querySelector('.carousel-btn.right');
      
      let current = 0;
      function showVideo(idx) {
        videos.forEach((vid, i) => {
          vid.style.display = (i === idx) ? 'block' : 'none';
          if (i === idx && vid.paused) vid.play();
          if (i !== idx && !vid.paused) vid.pause();
        });
      }
      btnLeft.addEventListener('click', () => {
        current = (current - 1 + videos.length) % videos.length;
        showVideo(current);
      });
      btnRight.addEventListener('click', () => {
        current = (current + 1) % videos.length;
        showVideo(current);
      });
      // Start with first video
      showVideo(current);
    });
  </script>

</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
          Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production.
          A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations.
          To address this, existing methods either train trajectory-conditioned video generation model on trajectory–video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions.
          Nevertheless, existing methods struggle to generate camera-pose–faithful, high-quality videos for two main reasons:
          (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and 
          (2) the limited diversity of camera trajectories in existing datasets restricts learned models.
          </br>
          To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity.
          The framework integrates two key components:
          (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and
          (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths.
          Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small" style="padding-left: 2.5vw; padding-right: 2.5vw;">
  <div class="hero-body" style="padding-left: 0; padding-right: 0;">
    <div class="container" style="max-width: 820px;">
      <h2 class="title is-3" style="font-size: 2rem;">Motivation</h2>
      <div class="columns">
        <div class="column is-full" style="padding-left: 1.5rem; padding-right: 1.5rem;">
          <figure class="image" style="margin-left: auto; margin-right: auto; max-width: 100%;">
            <img src="static/images/motivation.png" alt="Motivation" style="width: 100%; height: auto;">
          </figure>
          <p class="has-text-justified is-size-6 mt-3" style="font-size: 0.95rem;">
            <!-- (a) <strong>Proposed infinite homography-based approach.</strong> The model is conditioned on images warped by H∞, so it focuses on learning the parallax relative to the plane at infinity.
            This parallax is confined to the region between the epipole e′ and the point x∞ on the epipolar line l′ (shown as the yellow segment), which reduces the search space and leads to higher camera pose fidelity.
            End-to-end training further enables the network to implicitly refine the underlying 3D geometry, correcting inaccuracies in the unprojected 3D point X.
          </br>
            (b) <strong>Existing reprojection-based approach.</strong> Errors in depth estimation produce unreliable conditioning and cause artifacts in the generated image.
            Because no gradients flow back into the depth estimation network, the incorrect reprojection position x′ remains fixed during training, preventing these errors from being corrected. -->
            In the reprojection-based approach, inaccuracies in the depth estimation lead to unreliable conditioning, consequently introducing artifacts in the generated frame. 
            <!-- Since no gradients flow through the depth estimation network, the incorrect reprojection position x remains fixed during training, prohibiting error correction. -->
            <!-- INSERT_YOUR_CODE -->
            <!-- 수식 삽입 -->
            <!-- Use MathJax for proper display equation rendering -->            
            In contrast, based on the fact that reprojection can be expressed by the following equation,            
            <div style="margin-top: 1.3em; margin-bottom: 1.0em; text-align: center;">
              <img src="static/images/equation.png" alt="Equation: Reprojection formula" style="max-width: 60%; height: auto;">
            </div>
            our infinite homography-based approach conditions on noise-free frame warped by infinite homography. 
            This forces the model to concentrate on learning the parallax relative to the plane at infinity.
            This parallax is spatially constrained to the region between the epipole e' and the warped point x_inf, as visualized by the yellow segment on the epipolar line l’. 
            This constraint reduces the search space, enabling the model to achieve higher camera pose fidelity. 


          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small" style="padding-left: 2.5vw; padding-right: 2.5vw;">
  <div class="hero-body" style="padding-left: 0; padding-right: 0;">
    <div class="container" style="max-width: 820px;">
      <h2 class="title is-3" style="font-size: 2rem;">Approach</h2>

      <h3 class="subtitle is-4 mt-4" style="font-size: 1.35rem; font-weight: 600;">Model Architecture</h3>
      <!-- <div class="columns"> -->
      <div class="column is-full" style="padding-left: 1.5rem; padding-right: 1.5rem;">
        <figure class="image" style="margin-left: auto; margin-right: auto; max-width: 100%;">
          <img src="static/images/model_architecture.png" alt="Infinite Homography vs. Reprojection-based Conditioning" style="width: 100%; height: auto;">
        </figure>
        <p class="has-text-justified is-size-6 mt-3" style="font-size: 0.95rem;">
          (a) <strong>DiT block with homography-guided self-attention layer.</strong> The homography-guided self-attention layer takes source, target, and warped latents, combined with camera embeddings as input, and performs per-frame attention, ensuring temporal alignment. By conditioning on warped latents, the model enables rotation-aware reasoning and constrained parallax estimation. Only the source and target latents proceed to the subsequent Wan2.1 layers.<br>
          (b) <strong>Warping module.</strong> This module warps the input latent with infinite homography to handle rotation, then adds camera embeddings to account for translation. This decomposition simplifies reprojection to parallax estimation relative to the plane at infinity, enabling higher camera trajectory fidelity.
        </p>
      </div>
      <!-- data augmentation 영역에도 좌우 여백 적용 -->
      <h3 class="subtitle is-4 mt-5" style="font-size: 1.35rem; font-weight: 600;">Data Augmentation</h3>
      <!-- <h2 class="title is-3" style="font-size: 2rem;">Data Augmentation</h2> -->
      <div class="columns">
        <div class="column is-full" style="padding-left: 1.5rem; padding-right: 1.5rem;">
          <figure class="image" style="margin-left: auto; margin-right: auto; max-width: 90%;">
            <img src="static/images/data_preparation.png" alt="Model Architecture" style="width: 100%; height: auto;">
          </figure>
          <p class="has-text-justified is-size-6 mt-3" style="font-size: 0.95rem;">
            To remove bias from existing datasets, we augment the MultiCamVideo dataset, named AugMCV.
            Unlike the existing SynCamVideo and MultiCamVideo datasets, our AugMCV dataset includes camera trajectories with varying initial camera poses and different focal lengths.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Video Gallery -->
<section class="hero is-small">
  <div class="hero-body" style="padding-left: 1.5rem; padding-right: 1.5rem;">
    <div class="container" style="max-width: 820px; padding-left: 1.5rem; padding-right: 1.5rem;">
      <h2 class="title is-3">Experimental Results</h2>

      <!-- Qualitative Results (In-the-Wild) -->
      <h3 class="subtitle is-4 mt-5" style="font-size: 1.35rem; font-weight: 600;">Qualitative Comparison (AugMCV testset)</h3>
      <div class="columns is-multiline">
        <div class="column is-full" style="padding-left: 0; padding-right: 0;">
          <div style="display: flex; justify-content: center;">
            <video autoplay muted loop style="max-width: 820px; width: 80vw; height: auto; border-radius: 1rem;">
              <source src="static/videos/comparison_augmcv_testset.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <!-- Qualitative Results (WebVid Dataset) -->
      <h3 class="subtitle is-4 mt-5" style="font-size: 1.35rem; font-weight: 600;">Qualitative Comparison (In-the-Wild Dataset)</h3>
      <div class="columns is-multiline" id="webvid-carousel-container">
        <div class="column is-full" style="padding-left: 0; padding-right: 0;">
          <div style="display: flex; justify-content: center;">
            <video id="webvid-carousel" autoplay muted loop style="max-width: 820px; width: 80vw; height: auto; display: block; border-radius: 1rem;">
              <source src="static/videos/comparison_webvid_1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="carousel-controls" style="display: flex; justify-content: center; align-items: center; gap: 1.5rem; width: 100%; margin-top: 1rem;">
            <button class="carousel-btn left" id="webvid-prev"
              style="background:rgba(255,255,255,0.85); border:none; border-radius:50%; width:44px; height:44px; display:flex; align-items:center; justify-content:center; box-shadow:0 2px 8px rgba(0,0,0,0.09); transition: background 0.2s;">
              <svg height="26" width="26" viewBox="0 0 32 32" fill="none" stroke="#2563eb" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" style="display:block;">
                <circle cx="16" cy="16" r="15" fill="#fff" stroke="#e6eaf3" stroke-width="2"/>
                <polyline points="19,9 12,16 19,23" fill="none"/>
              </svg>
            </button>
            <button class="carousel-btn right" id="webvid-next"
              style="background:rgba(255,255,255,0.85); border:none; border-radius:50%; width:44px; height:44px; display:flex; align-items:center; justify-content:center; box-shadow:0 2px 8px rgba(0,0,0,0.09); transition: background 0.2s;">
              <svg height="26" width="26" viewBox="0 0 32 32" fill="none" stroke="#2563eb" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" style="display:block;">
                <circle cx="16" cy="16" r="15" fill="#fff" stroke="#e6eaf3" stroke-width="2"/>
                <polyline points="13,9 20,16 13,23" fill="none"/>
              </svg>
            </button>
          </div>
        </div>
      </div>
      <script>
        // Carousel videos: Add or update total count as needed
        const webvidVideos = [
          "static/videos/comparison_webvid_1.mp4",
          "static/videos/comparison_webvid_2.mp4",
          "static/videos/comparison_webvid_3.mp4",
          "static/videos/comparison_in-the-wild_1.mp4",
          "static/videos/comparison_in-the-wild_2.mp4",
        ];
        let webvidIndex = 0;

        const webvidVideoEl = document.getElementById("webvid-carousel");
        const webvidPrevBtn = document.getElementById("webvid-prev");
        const webvidNextBtn = document.getElementById("webvid-next");

        function updateWebvidVideo() {
          webvidVideoEl.pause();
          webvidVideoEl.querySelector("source").src = webvidVideos[webvidIndex];
          webvidVideoEl.load();
          webvidVideoEl.play();
        }

        webvidPrevBtn.addEventListener("click", () => {
          webvidIndex = (webvidIndex - 1 + webvidVideos.length) % webvidVideos.length;
          updateWebvidVideo();
        });

        webvidNextBtn.addEventListener("click", () => {
          webvidIndex = (webvidIndex + 1) % webvidVideos.length;
          updateWebvidVideo();
        });
      </script>

      <!-- Quantitative Results -->
      <!-- <h3 class="subtitle is-4 mt-5">Quantitative Results</h3> -->
      <h3 class="subtitle is-4 mt-5" style="font-size: 1.35rem; font-weight: 600;">Quantitative Comparison</h3>
      <div class="columns">
        <div class="column is-6" style="padding-left: 1.5rem; padding-right: 1.5rem;">
          <figure class="image">
            <img src="static/images/augmcv_result.png" alt="AugMCV result">
            <figcaption class="has-text-centered is-size-6 mt-2">
              <strong>AugMCV dataset</strong>
            </figcaption>
          </figure>
        </div>
        <div class="column is-6" style="padding-left: 1.5rem; padding-right: 1.5rem;">
          <figure class="image">
            <img src="static/images/webvid_result.png" alt="WebVid result">
            <figcaption class="has-text-centered is-size-6 mt-2">
              <strong>WebVid dataset</strong>
            </figcaption>
          </figure>
        </div>
      </div>

      <div class="columns">
        <div class="column is-full" style="padding-left: 1.5rem; padding-right: 1.5rem;">
          <p class="has-text-justified is-size-6 mt-3">
            <strong>AugMCV dataset.</strong> We evaluate our method under two scenarios: (1) source and target videos with identical camera intrinsics, and (2) source and target videos with different camera intrinsics. Across both settings and all metrics, our approach consistently outperforms the baselines, producing videos that are clearly closer to the ground truth.</br>
            <strong>WebVid dataset.</strong> We further validate our method on the WebVid dataset, where it again consistently outperforms baseline approaches in terms of both camera pose accuracy and visual fidelity, with particularly pronounced gains in camera pose accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video gallery -->


<!-- Comparison with Other Methods Gallery -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Comparison with Other Methods (Synthetic Dataset)</h2>
      <div class="columns is-multiline">
        <div class="column is-full">
          <video autoplay muted loop style="width: 100%; height: auto;">
            <source src="static/videos/Comparison_FF-Async_AugMCV.mp4" type="video/mp4">
          </video>
        </div>                      
      </div>      
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Comparison with Other Methods (WebVid Dataset)</h2>
      <div class="columns is-multiline">
        <div class="column is-full">
          <video autoplay muted loop style="width: 100%; height: auto;">
            <source src="static/videos/Comparison_FF-Async.mp4" type="video/mp4">
          </video>
        </div>   
        <div class="column is-full">
          <video autoplay muted loop style="width: 100%; height: auto;">
            <source src="static/videos/Comparison_FF-Sync.mp4" type="video/mp4">
          </video>
        </div>                          
      </div>    
      
    </div>
  </div>
</section> -->
<!-- End comparison gallery -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
